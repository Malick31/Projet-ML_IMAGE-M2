# -*- coding: utf-8 -*-
"""ProjetML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wQNd2UUN7qZhiCWPUHHCjWVglQgZYa2g

## Louzar- Ouniss-Sakho  M2-MAPI3

```



```
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import h5py

from keras.models import load_model
from tensorflow.keras.optimizers import Adam 

from google.colab import drive
drive.mount('/content/drive')

"""##Ouverture du ficher"""

lead_order = ["DI", "DII", "DIII", "AVL", "AVF", "AVR", "V1", "V2", "V3", "V4", "V5", "V6"]

path = '/content/drive/MyDrive'

df_goldStandard = pd.read_csv(f'{path}/data/annotations/gold_standard.csv')
df_goldStandard['nb_abnormality'] =  df_goldStandard.sum(axis=1)

df_goldStandard.head(8)
# On affiche les 8 premiers éléments du tableau afin de voir son contenu 
# goldStandar est obtenu lorsque le cardiologist 1 et 2 font le même diagnostic

df_goldStandard.describe()

df_goldStandard.nb_abnormality.value_counts()

df_attributes = pd.read_csv(f'{path}/data/attributes.csv')
df_attributes
df_attributes.hist()

# age et sexe des patients

"""Tracé des ECG pour l'étude réalisé"""

print(df_goldStandard.shape)
display(df_goldStandard.head(20).T)
with h5py.File(f'{path}/data/ecg_tracings.hdf5', "r") as f:
  x = np.array(f['tracings'])
print(x.shape)

model = load_model(f"{path}/model/model.hdf5", compile=False)
model.compile(loss='binary_crossentropy', optimizer=Adam())

for layer in model.layers:
  print(layer.output_shape)
print(model.summary())

import tensorflow as tf
from keras import backend as k


def find_last_conv(model):
  for layer in reversed(model.layers):
    if len(layer.output_shape) == 3:
      return layer.name


def grad_cam(model, time_serie, class_index, relative=True):
 
  
  grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(find_last_conv(model)).output, model.output])
  with tf.GradientTape() as tape:
    inputs = tf.cast([time_serie], tf.float32)
    conv_outputs, predictions = grad_model(inputs)
    loss = predictions[:, class_index]

  grads = tape.gradient(loss, conv_outputs)
  A = np.asarray(conv_outputs[0,:,:])
  dy = np.asarray(grads[0,:,:])
  a_k = dy.mean(axis=0)
  heat_map = np.sum(a_k*A, axis=1)
  heat_map = heat_map * (heat_map>0)
  
  if relative :
    heat_map /= heat_map.max()+1e-6
  return heat_map

from scipy.interpolate import interp1d

def plot_grad_cam_ecg(ts, check_y, selected_leads=True, relatives=True):
  hm = grad_cam(model, ts, check_y, relative=relatives)
  hm_interp = interp1d(np.linspace(0,10.24,16), hm, kind='slinear')
  if selected_leads:
    fig, axs = plt.subplots(1, 3,figsize=(40,5))
    for i,pos in enumerate([1,6,11]):
      ax = axs[i]
      ax.set_title(lead_order[pos],fontsize=10)
      ax.plot(np.linspace(0,10.24,4096),ts[:,pos],'k', linewidth=.5)
      t = ax.scatter(np.linspace(0,10.24,4096),ts[:,pos],linewidths=.1,c=hm_interp(np.linspace(0,10.24,4096)))
    fig.colorbar(t,ax=axs)
    plt.show()
    
  else :
    fig, axs = plt.subplots(4, 3,figsize=(20,16))
    for i in range(12):
      ax = axs[i//3, i%3]
      ax.set_title(lead_order[i],fontsize=10)
      ax.plot(np.linspace(0,10.24,4096),ts[:,pos],'k', linewidth=.5)
      t = ax.scatter(np.linspace(0,10.24,4096),ts[:,pos],linewidths=.1,c=hm_interp(np.linspace(0,10.24,4096)))
    fig.colorbar(t,ax=axs)
    plt.show()

for idx_abn, abnormality in enumerate(df_goldStandard.columns[:-1]):
  print(abnormality,idx_abn)

# visualisation a refaire en mieux
for idx_safe in df_goldStandard[df_goldStandard.nb_abnormality==0].index[:5]:
  fig, axs = plt.subplots(1, 3,figsize=(40,5))
  for i,pos in enumerate([1,6,11]):
    ax = axs[i]
    ax.set_title(lead_order[pos],fontsize=10)
    ax.plot(np.linspace(0,10.24,4096),x[idx_safe,:,pos],'k', linewidth=.5)
  plt.show()

for idx_abn, abnormality in enumerate(df_goldStandard.columns[:-1]):
  index_abnormality = df_goldStandard[(df_goldStandard[abnormality]==1)&(df_goldStandard.nb_abnormality==1)].index
  print(f'\n\n###########\t {abnormality}\t###########\n')
  preds = model.predict(x[index_abnormality[:3]])
  for i, pos in enumerate(index_abnormality[:3]):
    np.set_printoptions(precision=2,suppress=True)
    print(df_goldStandard.iloc[pos])
    print(preds[i])
    plot_grad_cam_ecg(x[pos],check_y=idx_abn)

#Pour visualiser un ECG
pos=259
ts = x[pos]
hm = grad_cam(model, ts, 4)
hm_interp = interp1d(np.linspace(0,10.24,16), hm, kind='slinear')

fig, ax= plt.subplots(1,1,figsize=(15,10))
ax.set_title('V6',fontsize=10)
ax.plot(np.linspace(0,10.24,4096),ts[:,11],'k', linewidth=.5)
t = ax.scatter(np.linspace(0,10.24,4096),ts[:,11],linewidths=.1,c=hm_interp(np.linspace(0,10.24,4096)))
fig.colorbar(t,ax=ax)
plt.savefig('V6.png', dpi=600)

import h5py
import math
import pandas as pd
from tensorflow.keras.utils import Sequence
import numpy as np


class ECGSequence(Sequence):
    @classmethod
    def get_train_and_val(cls, path_to_hdf5, hdf5_dset, path_to_csv, batch_size=8, val_split=0.02):
        n_samples = len(pd.read_csv(path_to_csv))
        n_train = math.ceil(n_samples*(1-val_split))
        train_seq = cls(path_to_hdf5, hdf5_dset, path_to_csv, batch_size, end_idx=n_train)
        valid_seq = cls(path_to_hdf5, hdf5_dset, path_to_csv, batch_size, start_idx=n_train)
        return train_seq, valid_seq

    def __init__(self, path_to_hdf5, hdf5_dset, path_to_csv=None, batch_size=8,
                 start_idx=0, end_idx=None):
        if path_to_csv is None:
            self.y = None
        else:
            self.y = pd.read_csv(path_to_csv).values
        # Get tracings
        self.f = h5py.File(path_to_hdf5, "r")
        self.x = self.f[hdf5_dset]
        self.batch_size = batch_size
        if end_idx is None:
            end_idx = len(self.x)
        self.start_idx = start_idx
        self.end_idx = end_idx

    @property
    def n_classes(self):
        return self.y.shape[1]

    def __getitem__(self, idx):
        start = self.start_idx + idx * self.batch_size
        end = min(start + self.batch_size, self.end_idx)
        if self.y is None:
            return np.array(self.x[start:end, :, :])
        else:
            return np.array(self.x[start:end, :, :]), np.array(self.y[start:end])

    def __len__(self):
        return math.ceil((self.end_idx - self.start_idx) / self.batch_size)

    def __del__(self):
        self.f.close()

! pip install datasets

"""On recupère les prédictions à l'aide de predict.py, on aurait aussi pu directement récupérer les dnn du model.hdf5"""

import numpy as np
import warnings
import argparse
warnings.filterwarnings("ignore")
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Get performance on test set from hdf5')
    parser.add_argument('path_to_hdf5', type=str,
                        help='path to hdf5 file containing tracings')
    parser.add_argument('path_to_model',  # or model_date_order.hdf5
                        help='file containing training model.')
    parser.add_argument('--dataset_name', type=str, default='tracings',
                        help='name of the hdf5 dataset containing tracings')
    parser.add_argument('--output_file', default="./dnn_output.npy",  # or predictions_date_order.csv
                        help='output csv file.')
    parser.add_argument('-bs', type=int, default=32,
                        help='Batch size.')

    args, unk = parser.parse_known_args([f'{path}/data/ecg_tracings.hdf5' ,f'{path}/model/model.hdf5'])
    if unk:
        warnings.warn("Unknown arguments:" + str(unk) + ".")

    # Import data
    seq = ECGSequence(args.path_to_hdf5, args.dataset_name, batch_size=args.bs)
    # Import model
    model = load_model(args.path_to_model, compile=False)
    model.compile(loss='binary_crossentropy', optimizer=Adam())
    y_score = model.predict(seq,  verbose=1)

    # Generate dataframe
    np.save(args.output_file, y_score)

    print("Output predictions saved")

y_pred = np.round(np.load('/content/dnn_output.npy'), 4)
y_pred
# on obtient les prédictions sous forme d'un array

df_pred = pd.DataFrame(y_pred,columns=['1dAVb','RBBB','LBBB','SB','AF','ST'])

df_pred.head(15)
#On converti y_pred en un dataframe 
# les résultats obtenus sont des probabilités

df_1 = np.asarray(df_pred['1dAVb'])
df_2 = np.asarray(df_pred['RBBB'])
df_3 = np.asarray(df_pred['LBBB'])
df_4 = np.asarray(df_pred['SB'])
df_5 = np.asarray(df_pred['AF'])
df_6 = np.asarray(df_pred['ST'])

# y true, ce sont les vraies prédictions
dgs1 = np.asarray(df_goldStandard['1dAVb'])
dgs2 = np.asarray(df_goldStandard['RBBB'])
dgs3 = np.asarray(df_goldStandard['LBBB'])
dgs4 = np.asarray(df_goldStandard['SB'])
dgs5 = np.asarray(df_goldStandard['AF'])
dgs6 = np.asarray(df_goldStandard['ST'])

from sklearn.metrics import f1_score
def bestThresshold(dsg1,df_1):
    best_thresh = None
    best_score = 0
    for thresh in np.arange(0.1, 0.501, 0.01):
        score = f1_score(dsg1, np.array(df_1)>thresh)
        if score > best_score:
            best_thresh = thresh
            best_score = score
    return best_score , best_thresh

print(bestThresshold(dgs1, df_1))
print(bestThresshold(dgs2, df_2))
print(bestThresshold(dgs3, df_3))
print(bestThresshold(dgs4, df_4))
print(bestThresshold(dgs5, df_5))
print(bestThresshold(dgs6, df_6))

# Il faut maintenant déterminer le meilleur seuil afin de trouver les résultats optimaux 
#On pourrait tester ce résultats pour plusieur seuil entre 0 et 1 mais on a le fichier dnn qui nous donne les meilleurs résultats 
#donc il est plus simple de l'utiliser

df_dnn = pd.read_csv(f'{path}/data/annotations/dnn.csv')

# y_pred, prédiction par le modèle de dnn 
df1 = np.asarray(df_dnn['1dAVb'])
df2 = np.asarray(df_dnn['RBBB'])
df3 = np.asarray(df_dnn['LBBB'])
df4 = np.asarray(df_dnn['SB'])
df5 = np.asarray(df_dnn['AF'])
df6 = np.asarray(df_dnn['ST'])

from sklearn.metrics import confusion_matrix, precision_recall_curve, accuracy_score

matrix = confusion_matrix(dgs1, df1)
print(matrix)
print('f1 scores', f1_score(dgs1, df1))
print('Le pourcentage d erreur pour l anomalie 1 est :', (((matrix[0][1] + matrix[1][0]) / 827) * 100 ) )

acc = accuracy_score(dgs1, df1)

print (acc)

"""Il faudrait ensuite faire de même pour les autres anomalies"""

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

fp1 , tp1, ts1 = roc_curve(dgs1, df1 )
fp2 , tp2, ts2 = roc_curve(dgs2, df2 )
fp3 , tp3, ts3 = roc_curve(dgs3, df3 )
fp4 , tp4, ts4 = roc_curve(dgs4, df4 )
fp5 , tp5, ts5 = roc_curve(dgs1, df5 )
fp6 , tp6, ts6 = roc_curve(dgs1, df1 )
def plot_roc_curve(fper, tper):
    plt.plot(fper, tper, color='red', label='ROC')
    plt.plot([0, 1], [0, 1], color='green', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

plot_roc_curve(fp1, tp1)
print("AUC pour l'anomalie 1 =" +str (roc_auc_score(dgs1, df1)))
plot_roc_curve(fp2, tp2)
print("AUC pour l'anomalie 2=" +str (roc_auc_score(dgs2, df2)))
plot_roc_curve(fp3, tp3)
print("AUC pour l'anomalie 3=" +str (roc_auc_score(dgs3, df3)))
plot_roc_curve(fp4, tp4)
print("AUC pour l'anomalie 4=" +str (roc_auc_score(dgs4, df4)))
plot_roc_curve(fp5, tp5)
print("AUC pour l'anomalie 5=" +str (roc_auc_score(dgs5, df5)))
plot_roc_curve(fp6, tp6)
print("AUC pour l'anomalie 6 =" +str (roc_auc_score(dgs6, df6)))

"""Deuxième partie transfert learning

## 1. Convert raw datasets from hospital centers into usable variables to train our model
"""

pip install wfdb

import pandas as pd
import numpy as np
import wfdb
import ast #package typed-ast

#Print versions
print('pandas version is :', pd.__version__)
print('numpy version is :', np.__version__)
print('wfdb version is :', wfdb.__version__)

! unzip '/content/drive/MyDrive/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip'

from tqdm import tqdm
def load_raw_data(df, sampling_rate, path):
    data = []
    if sampling_rate == 100:
        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]
    else:
        i=0
        for f in tqdm(df.filename_hr):
            data.append(wfdb.rdsamp(path+f))
            i+=1

    data = np.array([signal for signal, meta in data])
    return data

path_ = '/content/drive/MyDrive/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/'

sampling_rate=500

# load and convert annotation data
Y = pd.read_csv( path_ +'ptbxl_database.csv' , index_col='ecg_id')

Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))

Y = Y[0:5000]
#On, chosit les 5000 premières valeurs pour un problème 

# Load raw signal data
X = load_raw_data(Y, sampling_rate, path_)

# Load scp_statements.csv for diagnostic aggregation
agg_df = pd.read_csv(path_+'scp_statements.csv', index_col=0)
agg_df = agg_df[agg_df.diagnostic == 1]

def aggregate_diagnostic(y_dic):
    tmp = []
    for key in y_dic.keys():
        if key in agg_df.index:
            tmp.append(agg_df.loc[key].diagnostic_class)
    return list(set(tmp))



# Apply diagnostic superclass

Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)

# Split data into train and test
test_fold = 10
# Train
X_train = X[np.where(Y.strat_fold != test_fold)]
y_train = Y[(Y.strat_fold != test_fold)].diagnostic_superclass
# Test
X_test = X[np.where(Y.strat_fold == test_fold)]
y_test = Y[Y.strat_fold == test_fold].diagnostic_superclass

def delete_unk_ecg(x,y):
    y = list(y)
    x_processed,y_processed = [],[]
    for index,dis in enumerate(y):
        if dis != []:
            x_processed.append(x[index])
            y_processed.append(y[index])
    return x_processed,y_processed


X_train,y_train = delete_unk_ecg(X_train,y_train)
X_test,y_test = delete_unk_ecg(X_test,y_test)
            
y_train = np.array(y_train,dtype=object)
y_test = np.array(y_test,dtype=object)
X_train = np.array(X_train)
X_test = np.array(X_test)

print(y_train.shape)
print(X_train.shape)
print(X_test.shape)
print(y_test.shape)

#Define the diseases
diseases = ["NORM","MI","HYP","CD","STTC"]

#Create a RIBEIRO format training data
def Ribeiro_convert(y_train):
    
    y_train = list(y_train)

    y_train_ = [[0,0,0,0] for i in list(y_train)]
    for index, info in enumerate(y_train):

        for disease in info:

            if disease != 'NORM':
                y_train_[index][diseases.index(disease)-1] = 1

    y_train_ = np.array(y_train_)
    return y_train_

y_train = Ribeiro_convert(y_train)
y_test = Ribeiro_convert(y_test)

print(y_train.shape)
print(y_test.shape)


##Reformat the ECG aquisition, 4048 samples needed.

import cv2 # opencv
from tqdm import tqdm

def convert_X(X,scale):
    X_conv = np.empty((X.shape[0], 4096, 12))
    for index,array in enumerate(X):
        array = cv2.resize(array,(12,scale))
        X_conv[index,:,:]=array
    return X_conv
    
X_train_=convert_X(X_train,4096)    
X_test_=convert_X(X_test,4096)    

print(X_train.shape)
print(X_train_.shape)
print(X_test.shape)
print(X_test_.shape)

import numpy as np
import matplotlib.pyplot as plt


y = np.linspace(0,10,5000)
y_ =np.linspace(0,10,4096)


ObsID=7     #you can change the obsevations here
ChannelID=0 #you can change the channel here

plt.subplot(2, 1, 1)
plt.plot(y,X_train[ObsID,:,ChannelID])
plt.subplot(2, 1, 2)
plt.plot(y_,X_train_[ObsID,:,ChannelID])

print('y=',y_train[ObsID])

if (y_train[ObsID][0]==1):
    print("Disease = MI")
elif (y_train[ObsID][1]==1):
    print("Disease = HYP")
elif (y_train[ObsID][2]==1):
    print("Disease = CD")
elif (y_train[ObsID][3]==1):
    print("Disease = STTC")

"""# On effectue le transfert learning"""

from tensorflow import keras
from keras.models import load_model
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam 
from keras import Model

model_ = load_model(f"/content/drive/MyDrive/model/model.hdf5", compile=False) #on recupère le modèle 

model_.trainable = False #on gêle les couches

model1 = model_.layers[:-1][-1].output

modeltl = Dense(4096,activation='relu', trainable=True)(model1) #dernière couche du réseau modeltl est le modèle du transfert learning
modeltl = Dense(4,activation='sigmoid', trainable=True)(modeltl)

Modele = Model(inputs=model_.input, outputs=modeltl)

Modele.summary()

# On modifie les deux dernières couches car à la sortie on veut que 4 paramètre et non 6 ( c'est le but du transfert learning 
# modifier un réseau pour l'entrainer sur des nouvelles données )

#On va maintenant compiler le nouveau réseau en choisissant les epochs et le batch_size

Modele.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

Modele.fit(X_train_, y_train , epochs = 12 ,  batch_size = 50 )

y_pred1 = Modele.predict(X_test_)
y_pred1

df_pred1=pd.DataFrame(data = y_pred1, columns = ["MI","HYP","CD","STTC"])
df_pred1

df_test1=pd.DataFrame(data = y_test, columns = ["MI","HYP","CD","STTC"])
df_test1

pred1 = np.asarray(df_pred1['MI'])
pred2 = np.asarray(df_pred1['HYP'])
pred3 = np.asarray(df_pred1['CD'])
pred4 = np.asarray(df_pred1['STTC'])




test1 = np.asarray(df_test1['MI'])
test2 = np.asarray(df_test1['HYP'])
test3 = np.asarray(df_test1['CD'])
test4 = np.asarray(df_test1['STTC'])

from sklearn.metrics import f1_score
def bestThresshold(dsg1,df_1):
    best_thresh = None
    best_score = 0
    for thresh in np.arange(0.1, 0.501, 0.01):
        score = f1_score(dsg1, np.array(df_1)>thresh)
        if score > best_score:
            best_thresh = thresh
            best_score = score
    return best_score , best_thresh

print(bestThresshold(test1, pred1))
print(bestThresshold(test2, pred2))
print(bestThresshold(test3, pred3))
print(bestThresshold(test4, pred4))

#ON Choisit alors un seuil de 0.25
y_pred1 = Modele.predict(X_test_)
yprd = np.copy(y_pred1)
yprd[yprd > 0.25] = 1
yprd[yprd < 0.25] = 0

yprd # c'est le y_pred avec les bonnes probabilités et un seuil déterminé à l'aide de la fonction best-threshold

df_predf=pd.DataFrame(data = yprd, columns = ["MI","HYP","CD","STTC"])
df_predf

y_predf1 = np.asarray(df_predf['MI'])
y_predf2 = np.asarray(df_predf['HYP'])
y_predf3 = np.asarray(df_predf['CD'])
y_predf4 = np.asarray(df_predf['STTC'])



y_test1 = np.asarray(df_test1['MI'])
y_test2 = np.asarray(df_test1['HYP'])
y_test3 = np.asarray(df_test1['CD'])
y_test4 = np.asarray(df_test1['STTC'])

"""##**MI**"""

#Maintenant évaluont la performance du modèle à l 'aide de la matrice de confusion
#Pour MI On les résultats ci-dessous
matrix = confusion_matrix(y_test1, y_predf1)
print(matrix)
print('f1 scores', f1_score(y_test1, y_predf1))
print('Le pourcentage d erreur pour l anomalie  est :', (((matrix[0][1] + matrix[1][0]) / 827) * 100 ) )

"""## **HYP**"""

matrix = confusion_matrix(y_test2, y_predf2)
print(matrix)
print('f1 scores', f1_score(y_test2, y_predf2))
print('Le pourcentage d erreur pour l anomalie  est :', (((matrix[0][1] + matrix[1][0]) / 827) * 100 ) )

"""## **CD**"""

matrix = confusion_matrix(y_test3, y_predf3)
print(matrix)
print('f1 scores', f1_score(y_test3, y_predf3))
print('Le pourcentage d erreur pour l anomalie  est :', (((matrix[0][1] + matrix[1][0]) / 827) * 100 ) )

"""## **STTC**"""

matrix = confusion_matrix(y_test4, y_predf4)
print(matrix)
print('f1 scores', f1_score(y_test4, y_predf4))
print('Le pourcentage d erreur pour l anomalie  est :', (((matrix[0][1] + matrix[1][0]) / 827) * 100 ) )

"""On peut très bien construire un simple classifier tel que le SVC, mais la matrice de confusion nous permet déja de tirer des conclusions à savoir que le modèle pré entrainé ne fonctionne pas bien sur les nouvelles données et quil faudra modifier encore plus certaines couches du réseau de neurones afin d'en tirer le meilleur. On aurait aussi pu ne pas gêler les couches afin que les poids soient modifiés et que le réseau apprenne mieux lors de la backpropagation. """

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

fp1 , tp1, ts1 = roc_curve(y_test1, y_predf1 )
fp2 , tp2, ts2 = roc_curve(y_test2, y_predf2)
fp3 , tp3, ts3 = roc_curve(y_test3, y_predf3 )
fp4 , tp4, ts4 = roc_curve(y_test4, y_predf4 )

def plot_roc_curve(fper, tper):
    plt.plot(fper, tper, color='red', label='ROC')
    plt.plot([0, 1], [0, 1], color='green', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

plot_roc_curve(fp1, tp1)
print("AUC pour l'anomalie 1 =" +str (roc_auc_score(y_test1, y_predf1)))
plot_roc_curve(fp2, tp2)
print("AUC pour l'anomalie 2=" +str (roc_auc_score(y_test2, y_predf2)))
plot_roc_curve(fp3, tp3)
print("AUC pour l'anomalie 3=" +str (roc_auc_score(y_test3, y_predf3)))
plot_roc_curve(fp4, tp4)
print("AUC pour l'anomalie 4=" +str (roc_auc_score(y_test4, y_predf4)))

"""On remarque que les AUC sont presque toutes faibles et ne sont pas proche de 1. Ce qui montre à nouveau que le modèle ne prédit pas bien sur les nouvelles données"""

